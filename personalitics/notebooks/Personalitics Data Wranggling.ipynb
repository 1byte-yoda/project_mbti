{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling Personalitics Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.) Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import sqlite3\n",
    "from bs4 import BeautifulSoup\n",
    "from utils import parse_type_personality_cafe, unpack_topic_user, unpack_subcomments, \\\n",
    "                  unpack_subcomment_user, html_to_text, parse_type_16personality\n",
    "from utils import TYPES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../output/discussion_2.csv')\n",
    "df = df.rename({'id': 'topic_id'}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame Shape: (65171, 10)\n"
     ]
    }
   ],
   "source": [
    "print('DataFrame Shape:', df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.) Unpack the topic_user column then merge it into the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_users = pd.concat(df[['url', 'topic_user']].apply(unpack_topic_user, axis=1).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame Columns: Index(['comment_list', 'created_at', 'topic_id', 'project', 'server', 'spider',\n",
      "       'topic_post', 'topic_title', 'topic_user', 'url', 'avatar', 'user_name',\n",
      "       'user_type', 'posted_time_text', 'posted_datetime'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "df = pd.merge(df, df_users, how='inner', on='url')\n",
    "\n",
    "# Check the modified DF's columns\n",
    "print('DataFrame Columns:', df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../output/topic_discussion.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.) Unpack the comment_list (subcomment) column then merge it into the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../output/topic_discussion.csv')\n",
    "df['comment_list'] = df[['comment_list', 'url']].apply(unpack_subcomments, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subcomment = pd.concat(df['comment_list'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(605019, 25)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_subcomment.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'answerBody', 'answerBodyRaw', 'approved', 'approvedAtNice',\n",
       "       'approvedAtDiff', 'createdAtDiff', 'disapprovalReason',\n",
       "       'hasDisapprovalReason', 'hasUnreadSubComments', 'own', 'reportCount',\n",
       "       'reportedByUser', 'reportIgnored', 'reviewed', 'subCommentCount',\n",
       "       'totalVotingScore', 'unavailable', 'unread', 'updatedByUser',\n",
       "       'updatedByUserDiff', 'upvotedByUser', 'user', 'states', 'url'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_subcomment.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1.) Unpack the subcomment user column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_subcomment_user = df_subcomment[['user', 'id']].apply(unpack_subcomment_user, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subcomment = pd.merge(df_subcomment, df_subcomment_user, how='inner', on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subcomment.columns = list(map(lambda x: 'sub-'+ x, df_subcomment.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(605019, 36)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_subcomment.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subcomment.to_csv('../output/comment_discussion.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_merged = pd.merge(df, df_subcomment, how='inner', left_on='url', right_on='sub-url')\n",
    "# print('Merged DF Shape:', df_merged.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check the columns\n",
    "# df_merged.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.) Drop unecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_merged = df_merged.drop(['comment_list', 'topic_user', 'sub-user'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_merged.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_merged.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# old_mem_usage = df_merged.memory_usage().sum()\n",
    "# old_mem_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_merged['sub-id'] = df_merged['sub-id'].astype('int64')\n",
    "# df_merged['sub-approved'] = df_merged['sub-approved'].astype('int8')\n",
    "# df_merged['sub-createdAtDiff'] = df_merged['sub-createdAtDiff'].astype('int32')\n",
    "# df_merged['sub-reportCount'] = df_merged['sub-reportCount'].astype('int32')\n",
    "# df_merged['sub-reviewed'] = df_merged['sub-reviewed'].astype('int8')\n",
    "# df_merged['sub-subCommentCount'] = df_merged['sub-subCommentCount'].astype('int32')\n",
    "# df_merged['sub-totalVotingScore'] = df_merged['sub-totalVotingScore'].astype('int32')\n",
    "# df_merged['sub-unavailable'] = df_merged['sub-unavailable'].astype('int8')\n",
    "# df_merged['sub-upvotedByUser'] = df_merged['sub-upvotedByUser'].astype('bool')\n",
    "\n",
    "# df_merged['sub-updatedByUser'] = df_merged['sub-updatedByUser'].astype('bool')\n",
    "# df_merged['sub-unread'] = df_merged['sub-unread'].astype('bool')\n",
    "# df_merged['sub-reportIgnored'] = df_merged['sub-reportIgnored'].astype('bool')\n",
    "# df_merged['sub-reportedByUser'] = df_merged['sub-reportedByUser'].astype('bool')\n",
    "# df_merged['sub-own'] = df_merged['sub-own'].astype('bool')\n",
    "# df_merged['sub-hasUnreadSubComments'] = df_merged['sub-hasUnreadSubComments'].astype('bool')\n",
    "# df_merged['sub-hasDisapprovalReason'] = df_merged['sub-hasDisapprovalReason'].astype('bool')\n",
    "# df_merged['sub-disapprovalReason'] = df_merged['sub-disapprovalReason'].fillna('N/A')\n",
    "# df_merged['sub-states'] = df_merged['sub-states'].apply(lambda x: json.dumps(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_mem_usage = df_merged.memory_usage().sum()\n",
    "# new_mem_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# memory_saved = (old_mem_usage - new_mem_usage)/1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Total Memory Usage saved:', round(memory_saved, 2), 'mb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the modified DataFrame\n",
    "# df_merged.to_csv('../output/modified_discussion.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.) Aggregating text posts based on ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_data(source):\n",
    "    if source == 'personalitycafe':\n",
    "        conn = sqlite3.connect('../output/project_mbti.db')\n",
    "        df_db = pd.read_sql('SELECT * FROM personalitics', con=conn)\n",
    "        temp_df = pd.read_csv('../output/personality_cafe.csv')\n",
    "        temp_df = pd.concat([temp_df, df_db])\n",
    "        temp_df = temp_df[~(temp_df.duplicated())]\n",
    "        temp_df['child_text'] = temp_df['child_text'].fillna(' ')\n",
    "        temp_df = temp_df[~(temp_df['user_type'].isna())]\n",
    "        temp_df['user_type'] = temp_df['user_type'].apply(parse_type_personality_cafe)\n",
    "        temp_df = temp_df[(temp_df['user_type'].isin(TYPES))]\n",
    "        temp_df = temp_df[['user_id', 'child_text', 'user_type']].groupby(['user_id', 'user_type']).agg({'child_text': ' '.join}).reset_index()[['child_text', 'user_type']]\n",
    "        temp_df.to_csv('../output/aggregated/data_personalitycafe.csv', index=False)\n",
    "        \n",
    "    elif source == '16personalities_discussion_comments':\n",
    "        temp_df = pd.read_csv('../output/comment_discussion.csv')\n",
    "        temp_df = temp_df[['sub-id', 'sub-name', 'sub-profileUrl', 'sub-answerBody', 'sub-gender', 'sub-type', 'sub-url']]\n",
    "        temp_df = temp_df.drop(axis=1, index=temp_df[temp_df['sub-type'].isna()].index)\n",
    "        temp_df = temp_df.drop(axis=1, index=temp_df[temp_df['sub-answerBody'].isna()].index)\n",
    "        temp_df['sub-answerBody'] = temp_df['sub-answerBody'].apply(html_to_text)\n",
    "        temp_df = temp_df[['sub-profileUrl', 'sub-answerBody', 'sub-type']].groupby(['sub-profileUrl', 'sub-type']).agg({'sub-answerBody': ' '.join}).reset_index()[['sub-answerBody', 'sub-type']]\n",
    "        temp_df.to_csv('../output/aggregated/data_discussion_16personalities.csv', index=False)\n",
    "        \n",
    "    elif source == '16personalities_pub_comments':\n",
    "        temp_df = pd.read_csv('../output/sixteenpersonalities.csv')\n",
    "        temp_df = temp_df[~(temp_df['child_text'].isna())]\n",
    "        temp_df = temp_df[~(temp_df['user_type'].isna())]\n",
    "        temp_df['user_type'] = temp_df['user_type'].apply(parse_type_16personality)\n",
    "        temp_df = temp_df[(temp_df['user_type'].isin(TYPES))]\n",
    "        temp_df = temp_df[['user_id', 'child_text', 'user_type']].groupby(['user_id', 'user_type']).agg({'child_text': ' '.join}).reset_index()[['child_text', 'user_type']]\n",
    "        temp_df.to_csv('../output/aggregated/data_pub_16personalities.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregate_data('personalitycafe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2907: DtypeWarning: Columns (7) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n",
      "C:\\Users\\Razer\\Desktop\\project_mbti\\personalitics\\notebooks\\utils.py:42: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 42 of the file C:\\Users\\Razer\\Desktop\\project_mbti\\personalitics\\notebooks\\utils.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
      "\n",
      "  soup = BeautifulSoup(html.strip())\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:333: MarkupResemblesLocatorWarning: \".\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:421: MarkupResemblesLocatorWarning: \"https://www.youtube.com/watch?v=BR8_n-B8qu0\n",
      "https://www.youtube.com/watch?v=6_Y3zbRxZ6Q\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:421: MarkupResemblesLocatorWarning: \"http://www.breitbart.com/big-government/2016/07/11/black-cop-explodes-black-lives-matter-racist-police-myths-in-viral-facebook-post/\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:333: MarkupResemblesLocatorWarning: \"...\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:333: MarkupResemblesLocatorWarning: \"..\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:421: MarkupResemblesLocatorWarning: \"https://docs.google.com/spreadsheets/d/18BoEa_4vH3o_0oX_DRouDQ1YupvyFfXqqLhZaT8c0ng/edit?usp=sharing\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:421: MarkupResemblesLocatorWarning: \"http://www.vlive.tv/video/59950\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:421: MarkupResemblesLocatorWarning: \"https://m.youtube.com/watch?v=sMjj9_BICoE\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:421: MarkupResemblesLocatorWarning: \"https://www.youtube.com/watch?v=dtER80sOjX4\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:421: MarkupResemblesLocatorWarning: \"https://www.youtube.com/watch?v=dQw4w9WgXcQ\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:333: MarkupResemblesLocatorWarning: \"........................\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:421: MarkupResemblesLocatorWarning: \"https://giphy.com/gifs/animated-page-thread-vYTo95h4e2j6M\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:421: MarkupResemblesLocatorWarning: \"http://uploads.neatorama.com/images/posts/509/76/76509/1412949188-1.jpg\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:421: MarkupResemblesLocatorWarning: \"https://youtu.be/U_EupMUsb50\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:421: MarkupResemblesLocatorWarning: \"https://www.youtube.com/watch?v=o2oZWpqtNi4\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:421: MarkupResemblesLocatorWarning: \"https://youtu.be/UXp6dq5jg1g\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:421: MarkupResemblesLocatorWarning: \"https://m.youtube.com/watch?v=A2c1f4FE8cY\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:421: MarkupResemblesLocatorWarning: \"https://www.youtube.com/watch?v=cZvvrSqnnR8(bismil)\n",
      "https://www.youtube.com/watch?v=dQHphloYoPU(breathless)\n",
      "https://www.youtube.com/watch?v=Vy0roGA3hQ4(aashayein)\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:421: MarkupResemblesLocatorWarning: \"https://www.16personalities.com/articles/how-to-tell-if-someone-is-into-you-by-personality-type\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:333: MarkupResemblesLocatorWarning: \".....\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:421: MarkupResemblesLocatorWarning: \"https://youtu.be/PrmBnN7aQ9s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:333: MarkupResemblesLocatorWarning: \"c:\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:421: MarkupResemblesLocatorWarning: \"https://oxplore.org/question-detail/is-sleeping-more-important-than-studying\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:421: MarkupResemblesLocatorWarning: \"https://youtube.com/watch?v=OIF2-2xZ15M\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:421: MarkupResemblesLocatorWarning: \"https://www.stanleygibbons.com/dispatches\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:333: MarkupResemblesLocatorWarning: \". . .\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:421: MarkupResemblesLocatorWarning: \"https://www.youtube.com/watch?v=qNbaRiZJJQo\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:421: MarkupResemblesLocatorWarning: \"https://youtu.be/jUf1LpYlJ74\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:421: MarkupResemblesLocatorWarning: \"https://gfycat.com/CircularFairAfricanwildcat\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:421: MarkupResemblesLocatorWarning: \"https://www.treehugger.com/health/amp/8-reasons-why-you-should-go-walk.html\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:421: MarkupResemblesLocatorWarning: \"https://www.youtube.com/watch?v=gbw83FaIE1g&amp;list=PLU-TFjG0qugDOnndeSWGpQ_BTmzEBlK09&amp;index=17\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:421: MarkupResemblesLocatorWarning: \"https://am22.akamaized.net/tms/cnt/uploads/2010/07/chewy-nazi-squirrel1-e1280266068995.jpeg\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:421: MarkupResemblesLocatorWarning: \"https://www.deviantart.com/pixxus/art/Pallas-Cat-Griffon-834679463\n",
      ":3\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:421: MarkupResemblesLocatorWarning: \"https://youtu.be/3K9Ztpor-dQ\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:333: MarkupResemblesLocatorWarning: \"C:\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:421: MarkupResemblesLocatorWarning: \"http://llamafont.com/what\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:421: MarkupResemblesLocatorWarning: \"https://www.youtube.com/playlist?list=PLGBXNKgrgQslNw4NLqZkhkV1el0gu1xch\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:421: MarkupResemblesLocatorWarning: \"https://www.youtube.com/watch?v=twT1xbrOj_c\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:421: MarkupResemblesLocatorWarning: \"https://youtu.be/84dfjWjgaIo\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:421: MarkupResemblesLocatorWarning: \"https://www.wikihow.com/Cause-a-Person-to-Fall-Asleep\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:421: MarkupResemblesLocatorWarning: \"https://www.youtube.com/watch?v=Yp8zoatou2E\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:333: MarkupResemblesLocatorWarning: \"...........\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:421: MarkupResemblesLocatorWarning: \"https://youtu.be/g4QeypcBMyQ\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  MarkupResemblesLocatorWarning\n"
     ]
    }
   ],
   "source": [
    "aggregate_data('16personalities_discussion_comments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregate_data('16personalities_pub_comments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
