{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Personalitics Data Wranggling.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Xt02jlbgzUGx"
      ],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PHMark/project_mbti/blob/master/personalitics/notebooks/Personalitics_Data_Wranggling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2l_zIDIzUGZ",
        "colab_type": "text"
      },
      "source": [
        "# Data Wrangling Personalitics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wccogb0uzUGb",
        "colab_type": "text"
      },
      "source": [
        "## 0.) Load the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfgX8camzuih",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9bf626fb-66a4-497a-8d09-9a4415ec859b"
      },
      "source": [
        "!cp \"drive/My Drive/ML Projects/Personalitics/notebooks/utils.py\" ."
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cp: cannot stat 'drive/My Drive/ML Projects/Personalitics/notebooks/utils.py': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHY3RZPAzUGc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import re\n",
        "from datetime import datetime\n",
        "from scipy.stats import mode\n",
        "import sqlite3\n",
        "from bs4 import BeautifulSoup\n",
        "from tqdm import tqdm\n",
        "# from utils import parse_type_personality_cafe, unpack_topic_user, unpack_comments, \\\n",
        "#                   unpack_comment_user, html_to_text, parse_type_16personality\n",
        "# from utils import TYPES\n",
        "import os\n",
        "from dask import dataframe  as dd\n",
        "\n",
        "OUTPUT_DIR = r'drive/My Drive/ML Projects/Personalitics/output/'\n",
        "COMMENT_USER_DIR = OUTPUT_DIR + r'df_comment_user_chunks/'\n",
        "COMMENT_DIR = OUTPUT_DIR + r'df_comment_chunks/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srfJscWGE6ma",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWSlnLA_CipE",
        "colab_type": "text"
      },
      "source": [
        "<b> Unpacking json objects </b>\n",
        "<br>\n",
        "\n",
        "Here I will preprocess the dataset gathered from the 16Personalities forum. The total number of rows of this dataset is ~900k, and there are 14 columns namely, comment_list, url, id, topic, datetime & 9 more columns which I will not use for now. In total, the dimension of the dataset that I will use is 900k x 3 (comment_list, url, id).\n",
        "\n",
        "The comment column is in the form of a json object, inside this object is the text comment itself as well as a nested json which contains the information of the user that owns that comment. \n",
        "\n",
        "Here is the summarry of the json unpacking:\n",
        "\n",
        "1.) Load the dataset into chunks. I did this since my computer was not able to handle such large amount of load in its RAM.\n",
        "\n",
        "2.) Unpack the comment by loading it in the JSON reader and traversing through its keys. Then load the comments into a dataframe.\n",
        "\n",
        "3.) The same process applies for the user json.\n",
        "\n",
        "4.) Save the each chunked dataframe to.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNgA5t9szUGg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_chunks = pd.read_csv(OUTPUT_DIR + 'discussion_2.csv', chunksize=4000,\n",
        "                       usecols=['comment_list', 'url', 'id'])\n",
        "# List of columns of interest\n",
        "comment_user_cols = ['id', 'profileUrl', 'avatar', 'gender', 'reputation', 'type']\n",
        "comment_cols = ['id', 'approvedAtNice', 'answerBody', 'url']\n",
        "\n",
        "for i, chunk in tqdm(enumerate(df_chunks)):\n",
        "    # Unpack the comment json \n",
        "    chunk_comment = chunk[['comment_list', 'url']].apply(unpack_comments, axis=1)\n",
        "    chunk_comment = pd.concat(chunk_comment.values)\n",
        "    \n",
        "    # Unpack the comment_user json\n",
        "    chunk_comment_user = chunk_comment[['user', 'id']].apply(unpack_comment_user, axis=1)\n",
        "    \n",
        "\n",
        "    # Save into a csv file\n",
        "    chunk_comment_user[comment_user_cols].to_csv(COMMENT_USER_DIR + 'df_comment_user{}.csv'.format(i), index=False)\n",
        "    chunk_comment[comment_cols].to_csv(COMMENT_DIR + 'df_comment{}.csv'.format(i), index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lhqeMeLzUGk",
        "colab_type": "text"
      },
      "source": [
        "## 1.) Merging the user and the column dataframe chunks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0n9r4uvLVdt",
        "colab_type": "text"
      },
      "source": [
        "Since the user DataFrame chunks and the comment DataFrame chunks are associated with each other, I merged them into one DataFrame. Finally, I concatenated all of the DF chunks into a one big DataFrame, with a shape of 1.2m rows x 9 columns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "GvWMbEGizUGk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_comment_df():\n",
        "    tmp_df_ls = []\n",
        "    for file_user, file_comment in tqdm(zip(os.listdir(COMMENT_USER_DIR), os.listdir(COMMENT_DIR))):\n",
        "        curr_file_user = os.path.join(COMMENT_USER_DIR, file_user)\n",
        "        curr_file_comment = os.path.join(COMMENT_DIR, file_comment)\n",
        "\n",
        "        # Load the csv files into a DataFrame\n",
        "        tmp_df_comments_user = pd.read_csv(curr_file_user)\n",
        "        tmp_df_comment = pd.read_csv(curr_file_comment)\n",
        "\n",
        "        # Merge the DataFrames\n",
        "        tmp_df = pd.merge(tmp_df_comment, tmp_df_comments_user, how='inner', on='id')\n",
        "        tmp_df_ls.append(tmp_df)\n",
        "        \n",
        "    final_df = pd.concat(tmp_df_ls)\n",
        "    return final_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOsrOWa9zUGn",
        "colab_type": "code",
        "outputId": "a2df3f4d-a458-4998-a7bc-b2b0bf1bd8fa",
        "colab": {}
      },
      "source": [
        "df_merged = get_comment_df()\n",
        "df_merged.columns = list(map(lambda x: 'sub-'+ x, df_merged.columns))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "32it [00:12,  2.56it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIINgWHVzUGs",
        "colab_type": "code",
        "outputId": "a58022e3-49ce-4041-9c1c-d41f5f4164e1",
        "colab": {}
      },
      "source": [
        "df_merged.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1169475, 9)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHeJKuSpzUGv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_merged.to_csv(OUTPUT_DIR + r'comment_discussion.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xt02jlbgzUGx",
        "colab_type": "text"
      },
      "source": [
        "## 2.) Tweak memory usage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVPlQ1YuzUGy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# df_merged['sub-id'] = df_merged['sub-id'].astype('int64')\n",
        "# df_merged['sub-approved'] = df_merged['sub-approved'].astype('int8')\n",
        "# df_merged['sub-createdAtDiff'] = df_merged['sub-createdAtDiff'].astype('int32')\n",
        "# df_merged['sub-reportCount'] = df_merged['sub-reportCount'].astype('int32')\n",
        "# df_merged['sub-reviewed'] = df_merged['sub-reviewed'].astype('int8')\n",
        "# df_merged['sub-subCommentCount'] = df_merged['sub-subCommentCount'].astype('int32')\n",
        "# df_merged['sub-totalVotingScore'] = df_merged['sub-totalVotingScore'].astype('int32')\n",
        "# df_merged['sub-unavailable'] = df_merged['sub-unavailable'].astype('int8')\n",
        "# df_merged['sub-upvotedByUser'] = df_merged['sub-upvotedByUser'].astype('bool')\n",
        "\n",
        "# df_merged['sub-updatedByUser'] = df_merged['sub-updatedByUser'].astype('bool')\n",
        "# df_merged['sub-unread'] = df_merged['sub-unread'].astype('bool')\n",
        "# df_merged['sub-reportIgnored'] = df_merged['sub-reportIgnored'].astype('bool')\n",
        "# df_merged['sub-reportedByUser'] = df_merged['sub-reportedByUser'].astype('bool')\n",
        "# df_merged['sub-own'] = df_merged['sub-own'].astype('bool')\n",
        "# df_merged['sub-hasUnreadSubComments'] = df_merged['sub-hasUnreadSubComments'].astype('bool')\n",
        "# df_merged['sub-hasDisapprovalReason'] = df_merged['sub-hasDisapprovalReason'].astype('bool')\n",
        "# df_merged['sub-disapprovalReason'] = df_merged['sub-disapprovalReason'].fillna('N/A')\n",
        "# df_merged['sub-states'] = df_merged['sub-states'].apply(lambda x: json.dumps(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QQ2AGvjzUG0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# new_mem_usage = df_merged.memory_usage().sum()\n",
        "# new_mem_usage"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HW5bJ_I_zUG2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# memory_saved = (old_mem_usage - new_mem_usage)/1000000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDbFZkexzUG4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print('Total Memory Usage saved:', round(memory_saved, 2), 'mb')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qP0JNOFhzUG6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Save the modified DataFrame\n",
        "# df_merged.to_csv('../output/modified_comment.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bgy90eIqzUG8",
        "colab_type": "text"
      },
      "source": [
        "## 3.) Aggregating text posts based on User ID"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6VkHPiAzUG8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_date(x):\n",
        "    try:\n",
        "        return re.sub('\\..+', '', x)\n",
        "    except:\n",
        "        print(x)\n",
        "\n",
        "def strip_time(dtime):\n",
        "  dt = dtime['created_at'].str\n",
        "  dt = dt.replace(r' \\d+:\\d+:\\d+', '')\n",
        "  dt = +  ' ' + dtime[ 'date'].astype(str)\n",
        "  return dt\n",
        "\n",
        "def aggregate_data(source):\n",
        "    if source == 'personalitycafe':\n",
        "        conn = sqlite3.connect(OUTPUT_DIR + 'project_mbti.db')\n",
        "        query = '''SELECT user_id, user_type, child_text, date, \n",
        "                   created_at FROM personalitics'''\n",
        "        temp_cols = ['user_id', 'user_type', 'child_text', 'date', 'created_at']\n",
        "        final_cols = ['user_id', 'child_text', 'date', 'dow', 'user_type']\n",
        "\n",
        "        # Load the DataFrames\n",
        "        df_db = pd.read_sql(query, con=conn)\n",
        "        temp_df = pd.read_csv(OUTPUT_DIR + 'personality_cafe.csv', \n",
        "                              usecols=temp_cols)\n",
        "        \n",
        "        # Concatenate each DataFrame and Remove Duplicates\n",
        "        temp_df = pd.concat([temp_df, df_db])\n",
        "        temp_df = temp_df[~(temp_df.duplicated())]\n",
        "\n",
        "        # Remove NULL values\n",
        "        temp_df = temp_df[~(temp_df['user_type'].isna())]\n",
        "        temp_df = temp_df[~(temp_df['child_text'].isna())]\n",
        "\n",
        "        # Normalize the type column ie. Parse only the first 4 characters from (INTP-A)\n",
        "        temp_df['user_type'] = temp_df['user_type'].apply(parse_type_personality_cafe)\n",
        "        temp_df = temp_df[(temp_df['user_type'].isin(TYPES))]\n",
        "\n",
        "        # Trim the datetime column\n",
        "        has_yesterday = temp_df['date'].str.contains(r'Yesterday|Today')\n",
        "        temp_df.loc[has_yesterday, 'date'] = temp_df.loc[has_yesterday, 'date'].str.strip('Yesterday ').str.strip('Today ')\n",
        "        temp_df['created_at'] = temp_df['created_at'].apply(get_date)\n",
        "        temp_df.loc[has_yesterday, 'date'] = temp_df.loc[has_yesterday, ['date', 'created_at']].apply(strip_time, axis=1)\n",
        "        temp_df['date'] = pd.to_datetime(temp_df['date'])\n",
        "\n",
        "        # Create a DayofWeek column and Concatenate all comments with |||\n",
        "        temp_df['dow'] = temp_df['date'].copy()\n",
        "        temp_df = temp_df[final_cols].groupby(['user_id', 'user_type'])\n",
        "        agg_func = {'date': lambda x: x.dt.hour.median(),\n",
        "                    'child_text': '|||'.join,\n",
        "                    'dow': lambda z: mode(z.dt.dayofweek).mode[0]}\n",
        "        temp_df = temp_df.agg(agg_func)\n",
        "        temp_df = temp_df.reset_index()\n",
        "\n",
        "        # Save DF\n",
        "        temp_df.to_csv(OUTPUT_DIR + 'aggregated/data_personalitycafe.csv', \n",
        "                       index=False)\n",
        "        \n",
        "    elif source == '16personalities_discussion_comments':\n",
        "        # Read CSV into a DataFrame\n",
        "        temp_df = pd.read_csv(OUTPUT_DIR + 'discussion_comments_16personalities.csv')\n",
        "\n",
        "        # Remove all NULL Values\n",
        "        temp_df = temp_df[~temp_df['sub-answerBody'].isna()]\n",
        "\n",
        "        # Create a DayofWeek column and Concatenate all comments with |||\n",
        "        agg_func = {'sub-answerBody': '|||'.join, \n",
        "                    'sub-approvedAtNice': lambda x: x.median(),\n",
        "                    'sub-dow': lambda z: mode(z.values)[0]}\n",
        "        temp_df = temp_df.groupby(['sub-profileUrl',\t'sub-type']).agg(agg_func)\n",
        "        temp_df = temp_df.reset_index()\n",
        "\n",
        "        # Save DF\n",
        "        temp_df.to_csv(OUTPUT_DIR + 'aggregated/data_discussion_16personalities.csv',\n",
        "                       index=False)\n",
        "        \n",
        "    elif source == '16personalities_pub_comments':\n",
        "        final_cols = ['user_id', 'child_text', 'date', 'dow', 'user_type']\n",
        "\n",
        "        # Read CSV into a DataFrame\n",
        "        temp_df = pd.read_csv(OUTPUT_DIR + 'sixteenpersonalities.csv')\n",
        "\n",
        "        # Remove all NULL values\n",
        "        temp_df = temp_df[~(temp_df['child_text'].isna())]\n",
        "        temp_df = temp_df[~(temp_df['user_type'].isna())]\n",
        "\n",
        "        # Normalize the type column ie. Parse only the first 4 characters from (INTP-A)\n",
        "        temp_df['user_type'] = temp_df['user_type'].apply(parse_type_16personality)\n",
        "        temp_df = temp_df[(temp_df['user_type'].isin(TYPES))]\n",
        "\n",
        "        # Covert the date column into a datetime datatype\n",
        "        temp_df['date'] = pd.to_datetime(temp_df['date'])\n",
        "\n",
        "        # Create a DayofWeek column and Concatenate all comments with |||\n",
        "        temp_df['dow'] = temp_df['date'].copy()\n",
        "        temp_df = temp_df[final_cols].groupby(['user_id', 'user_type'])\n",
        "        agg_func = {'date': lambda x: x.dt.hour.median(),\n",
        "                    'child_text': '|||'.join,\n",
        "                    'dow': lambda z: mode(z.dt.dayofweek).mode[0]}\n",
        "        temp_df = temp_df.agg(agg_func)\n",
        "        temp_df = temp_df.reset_index()\n",
        "\n",
        "        # Save DF\n",
        "        temp_df.to_csv(OUTPUT_DIR + 'aggregated/data_pub_16personalities.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "Npy5ifPvzUG-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "aggregate_data('personalitycafe')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YEFy0FFEzUHA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "aggregate_data('16personalities_discussion_comments')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHBKiqRjzUHC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "aggregate_data('16personalities_pub_comments')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}